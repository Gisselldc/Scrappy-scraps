import spacy
import os
import csv
from textblob import TextBlob  # For sentiment analysis

# Load spaCy's English model
nlp = spacy.load("en_core_web_sm")

# Define input and output directories
input_directory = "/Users/gisselldc/Comp Docs/Blogs Project/Open AI/News"  # Folder with your text files
output_csv = "/Users/gisselldc/Comp Docs/Blogs Project/Open AI/News/processed/entities_and_features.csv"  # CSV file to save results

# Prepare the CSV header
header = ['File Name', 'Entity Text', 'Entity Type', 'POS', 'Dependency', 'Sentence', 'Noun Chunk', 'Sentiment Polarity', 'Sentiment Subjectivity', 'Lemmatized Tokens', 'Verbs', 'Adjectives', 'Adverbs']

# Open the CSV file for writing
with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:
    writer = csv.writer(csv_file)
    writer.writerow(header)  # Write the header row

    # Process each file in the directory
    for filename in os.listdir(input_directory):
        if filename.endswith(".txt"):  # Process only text files
            file_path = os.path.join(input_directory, filename)

            # Read the file content
            with open(file_path, "r", encoding="utf-8") as file:
                text = file.read()

            # Process the text with spaCy
            doc = nlp(text)

            # Perform sentiment analysis with TextBlob
            blob = TextBlob(text)
            sentiment_polarity = blob.sentiment.polarity  # Sentiment polarity
            sentiment_subjectivity = blob.sentiment.subjectivity  # Sentiment subjectivity

            # Extract Named Entities and other features
            for ent in doc.ents:  # For each entity in the document
                # Start with basic entity info (12 columns initialized)
                row = [filename, ent.text, ent.label_, "", "", "", "", sentiment_polarity, sentiment_subjectivity, "", "", "", ""]

                # Extract Part-of-Speech tags and Dependencies for each token in the entity's sentence
                for sent in doc.sents:
                    if ent.start_char >= sent.start_char and ent.end_char <= sent.end_char:
                        for token in sent:
                            if token.text == ent.text:
                                row[3] = token.pos_  # POS of the entity
                                row[4] = token.dep_  # Dependency of the entity
                                row[5] = sent.text  # Full sentence containing the entity

                # Extract Noun Chunks
                noun_chunks = [chunk.text for chunk in doc.noun_chunks]
                row[6] = "; ".join(noun_chunks)  # Join noun chunks as a string

                # Extract Lemmatized Tokens
                lemmatized_tokens = [token.lemma_ for token in doc]
                row[9] = "; ".join(lemmatized_tokens)  # Join lemmatized tokens as a string

                # Extract Verbs, Adjectives, and Adverbs
                verbs = [token.text for token in doc if token.pos_ == 'VERB']
                adjectives = [token.text for token in doc if token.pos_ == 'ADJ']
                adverbs = [token.text for token in doc if token.pos_ == 'ADV']

                # Ensure the row is properly extended (ensure all columns are filled)
                row[10] = "; ".join(verbs) if verbs else "None"  # If no verbs, set as "None"
                row[11] = "; ".join(adjectives) if adjectives else "None"  # If no adjectives, set as "None"
                row[12] = "; ".join(adverbs) if adverbs else "None"  # If no adverbs, set as "None"

                # Write the row to CSV
                writer.writerow(row)

            print(f"âœ… Processed: {filename}")

print(f"ğŸ‰ All documents processed and saved to {output_csv}")
